{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_url=(\"http://www.boxofficemojo.com/movies/alphabetical.htm?letter=NUM&p=.html\")# starting point for search, can be any letter\n",
    "movie_links=[]#initialize as an empty list\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(urlopen(current_url).read())\t#generate list of links for the letter indices\n",
    "letters = soup.findAll('a', href= re.compile('letter='))\n",
    "letter_index = [] #intialize as an empty list\n",
    "for t in letters:\n",
    "    letter_index.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "\n",
    "for i in range(0,27): #loop through all letter indices for movies\n",
    "    current_url = letter_index[i]\n",
    "    soup = BeautifulSoup(urlopen(current_url).read())\n",
    "    navbar = soup.find('div', 'alpha-nav-holder')\n",
    "    pages = navbar.findAll('a', href= re.compile('alphabetical'))\n",
    "    page_list = [] # page_list is reset for each letter index\n",
    "\n",
    "    for t in pages:\n",
    "        page_list.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "\n",
    "\n",
    "    movietable = soup.find('div',{'id':'main'})\n",
    "    movies = movietable.findAll('a', href= re.compile('id='))\n",
    "    for t in movies:\n",
    "        movie_links.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "\n",
    "    if pages != None: #this only runs if there is a 2nd page for this letter\n",
    "        i=0\t#page list starts at 2 (consequence of page layout)\n",
    "        while i<len(page_list): #loop over multiple pages for each letter index\n",
    "            current_url = page_list[i] \n",
    "            soup = BeautifulSoup(urlopen(current_url).read())\n",
    "            movietable = soup.find('div',{'id':'main'})\n",
    "            movies = movietable.findAll('a', href= re.compile('id='))\n",
    "            for t in movies:\n",
    "                movie_links.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "            i+=1\n",
    "\n",
    "\n",
    "with open(\"movie_data.csv\", \"w\") as f:\n",
    "    fieldnames = (\"title\",\"director1\",\"director2\",\"domestic\", \"distributor\",\"release\",\"genre\",\"runtime\",\"rating\",\"budget\",\"worldwide\",\"actor1\",\"actor2\",\"actor3\",\"actor4\",\"actor5\",\"actor6\",\"producer1\",\"producer2\",\"producer3\",\"producer4\",\"producer5\",\"producer6\",\"writer1\",\"writer2\",\"composer1\",\"composer2\")#can add foreign release dates/boxoffice, review data, award data, trope data from other sites eventually\n",
    "    output = csv.writer(f, delimiter=\",\")\n",
    "    output.writerow(fieldnames)\n",
    "\n",
    "    for url in movie_links:\n",
    "        if \"elizabeth\" in url and \"elizabethtown\" not in url:#fixes an annoying encoding error in an inelagent way\n",
    "            url = 'http://www.boxofficemojo.com/movies/?id=elizabeth%A0.htm'\n",
    "        if \"simpleplan\" in url:\n",
    "            url = 'http://www.boxofficemojo.com/movies/?id=simpleplan%A0.htm'\n",
    "        try:\n",
    "        #print url\n",
    "        #time.sleep(0.1) #pause for courtesy? not sure if neccesary,i'm new at this\n",
    "            current_url = (url + \"&adjust_yr=2017&p=.htm\")  #do all movies in 2015 dollars (done automatically by site with correct URL)\n",
    "\n",
    "            soup = BeautifulSoup(urlopen(current_url).read())\n",
    "\n",
    "            directors = soup.findAll('a', href= re.compile('Director&id'))\n",
    "            director_list = []\n",
    "            for t in directors:\n",
    "                director_list.append(t.encode_contents())\n",
    "            for i in range(0,2):\n",
    "                if i>=len(director_list):\n",
    "                    director_list.append('N/A')#fill rest of list\n",
    "            director1 = director_list[0]\n",
    "            director2 = director_list[1]\n",
    "\n",
    "            writers = soup.findAll('a', href= re.compile('Writer&id'))\n",
    "            writer_list = []\n",
    "            for t in writers:\n",
    "                writer_list.append(t.encode_contents())\n",
    "            for i in range(0,2):\n",
    "                if i>=len(writer_list):\n",
    "                    writer_list.append('N/A')\n",
    "            writer1 = writer_list[0]\n",
    "            writer2 = writer_list[1]\n",
    "\n",
    "            composers=soup.findAll('a', href= re.compile('Composer&id'))\n",
    "            composer_list = []\n",
    "            for t in composers:\n",
    "                composer_list.append(t.encode_contents())\n",
    "            for i in range(0,2):\n",
    "                if i>=len(composer_list):\n",
    "                    composer_list.append('N/A')\n",
    "            composer1 = composer_list[0]\n",
    "            composer2 = composer_list[1]\n",
    "\n",
    "            actors=soup.findAll('a', href= re.compile('Actor&id'))\n",
    "            actor_list = []\n",
    "            for t in actors:\n",
    "                actor_list.append(t.encode_contents())\n",
    "            for i in range(0,6):\n",
    "                if i>=len(actor_list):\n",
    "                    actor_list.append('N/A')\n",
    "            actor1 = actor_list[0]\n",
    "            actor2 = actor_list[1]\n",
    "            actor3 = actor_list[2]\n",
    "            actor4 = actor_list[3]\n",
    "            actor5 = actor_list[4]\n",
    "            actor6 = actor_list[5]\n",
    "\n",
    "            producers = soup.findAll('a', href= re.compile('Producer&id'))\n",
    "            producer_list = []\n",
    "            for t in producers:\n",
    "                producer_list.append(t.encode_contents())\n",
    "            for i in range(0,6):\n",
    "                if i>=len(producer_list):\n",
    "                    producer_list.append('N/A')\n",
    "            producer1 = producer_list[0]\n",
    "            producer2 = producer_list[1]\n",
    "            producer3 = producer_list[2]\n",
    "            producer4 = producer_list[3]\n",
    "            producer5 = producer_list[4]\n",
    "            producer6 = producer_list[5]\n",
    "\n",
    "\n",
    "            all_bs = soup.findAll('b')\n",
    "            b_list = [] #lots of the information we want is in bold, and appears in the same order on each page\n",
    "            for t in all_bs:\n",
    "                if 'Domestic Lifetime' not in str(t.encode_contents(),'utf-8'):#want to ignore the lifetime box office\n",
    "                    b_list.append(str(t.encode_contents(),'utf-8'))\n",
    "            if len(b_list)>=10:#avoids bad entries with no box office data\n",
    "                if '$'in b_list[2] or 'n/a' in b_list[9]:#avoid movies w/o box office data, or unadjustable box office data, if not caught above\n",
    "                    if 'n/a' in b_list[9]:#has a foreign release only, order is shifted\n",
    "                        title = b_list[1]\n",
    "                        domestic = 'N/A'\n",
    "                        if 'N/A' not in b_list[2]:\n",
    "                            distributor = b_list[2].split('>')[1].split('<')[0]\n",
    "                        else:\n",
    "                            distributor = b_list[2]\n",
    "                        if len(b_list[3].split('>'))>3:#sometimes the release date is not in a hyperlink\n",
    "                            release = b_list[3].split('>')[2].split('<')[0]\n",
    "                        else:\n",
    "                            release = b_list[3].split('>')[1].split('<')[0]\n",
    "                        genre = b_list[4]\n",
    "                        runtime = b_list[5]\n",
    "                        rating = b_list[6]\n",
    "                        budget = b_list[7]\n",
    "                        worldwide = b_list[12]\n",
    "                    else:\t#has a domestic release\n",
    "                        title = b_list[1]\n",
    "                        domestic = b_list[2]\n",
    "                        if 'n/a' not in b_list[3]:\n",
    "                            distributor = b_list[3].split('>')[1].split('<')[0]\n",
    "                        else:\n",
    "                            distributor = b_list[3]\n",
    "                        if len(b_list[4].split('>'))>3:#sometimes the release date is not in a hyperlink\n",
    "                            release = b_list[4].split('>')[2].split('<')[0]\n",
    "                        else:\n",
    "                            release = b_list[4].split('>')[1].split('<')[0]\n",
    "                        genre = b_list[5]\n",
    "                        runtime = b_list[6]\n",
    "                        rating = b_list[7]\n",
    "                        budget = b_list[8]\n",
    "                        if len(b_list)==11 or '%' not in b_list[11]:#this means it only has a domestic release\n",
    "                            worldwide = 'N/A'\n",
    "                        else:\n",
    "                            worldwide = b_list[13]\n",
    "                \n",
    "                print(release)\n",
    "                output.writerow([title,director1,director2,domestic,distributor,release,genre,runtime,rating,budget,worldwide,actor1,actor2,actor3,actor4,actor5,actor6,producer1,producer2,producer3,producer4,producer5,producer6,writer1,writer2,composer1,composer2])#since this is in the big \"if\" it wont write to file if it is formated incorrectly\n",
    "        except:\n",
    "            pass\n",
    "\t\t#else:\n",
    "\t\t\t#print 'bad format'\n",
    "\t\t\t#print url\t#i want to see which ones i threw away, to check if my criteria is good\n",
    "\t\t\t#print 'bad format'\n",
    "\n",
    "#print(\"Done writing file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movie = pd.read_csv(\"movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movie[df_movie.title=='Pirates of the Caribbean: Dead Men Tell No Tales']\n",
    "df_movie_rank = df_rank.merge(df_movie, on='title', how='left')\n",
    "df_movie_rank.to_csv('movie_rank.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
